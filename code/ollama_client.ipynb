{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be124d0",
   "metadata": {},
   "source": [
    "### Creación de un cliente MCP local con LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b0a7e7",
   "metadata": {},
   "source": [
    "Este Jupyter Notebook te guía en la creación de un cliente MCP (Protocolo de Contexto de Modelo) local que puede comunicarse con una base de datos mediante herramientas expuestas por un servidor MCP, completamente en tu equipo. Sigue las celdas en orden para un tutorial fluido e independiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5b29e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa nest_asyncio para permitir la ejecución de bucles de eventos anidados en notebooks\n",
    "import nest_asyncio\n",
    "# Aplica el parche para que asyncio funcione correctamente en entornos interactivos como Jupyter\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4002d01",
   "metadata": {},
   "source": [
    "#### Configurar una LLM local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4c905405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa la clase Ollama para usar modelos LLM locales\n",
    "from llama_index.llms.ollama import Ollama\n",
    "# Importa Settings para configurar el modelo globalmente en LlamaIndex\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Crea una instancia del modelo LLM local especificando el modelo y el timeout\n",
    "llm = Ollama(model=\"llama3.2\", request_timeout=120.0)\n",
    "# Asigna el modelo creado a la configuración global de LlamaIndex\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7027786f",
   "metadata": {},
   "source": [
    "#### Inicializar el cliente MCP y crear el agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4671d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa las clases necesarias para crear un cliente MCP y especificar herramientas\n",
    "from llama_index.tools.mcp import BasicMCPClient, McpToolSpec\n",
    "\n",
    "# Inicializa el cliente MCP apuntando al endpoint del servidor MCP local\n",
    "mcp_client = BasicMCPClient(\"http://localhost:8000/sse\")\n",
    "# Crea el objeto de herramientas MCP, que puede filtrar o limitar las herramientas disponibles\n",
    "mcp_tools = McpToolSpec(client=mcp_client)  # También puedes pasar una lista de herramientas permitidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bf5b28a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_person \n",
      "    Inserta un nuevo registro en la tabla 'people' de la base de datos.\n",
      "\n",
      "    Permite agregar personas a la base de datos proporcionando nombre, edad y profesión.\n",
      "\n",
      "    Parámetros:\n",
      "        name (str): Nombre de la persona.\n",
      "        age (int): Edad de la persona.\n",
      "        profession (str): Profesión de la persona.\n",
      "\n",
      "    Retorna:\n",
      "        str: Mensaje de éxito o error.\n",
      "\n",
      "    Ejemplo de uso:\n",
      "        >>> add_person('Cristiano Ronaldo', 41, 'Football Player')\n",
      "        \"Persona 'Cristiano Ronaldo' agregada exitosamente\"\n",
      "    \n",
      "get_people \n",
      "    Lee datos de la tabla 'people' de la base de datos usando una consulta SQL SELECT.\n",
      "\n",
      "    Permite recuperar registros de la tabla 'people' según la consulta SQL proporcionada. \n",
      "    Por defecto, devuelve todos los registros.\n",
      "\n",
      "    Parámetros:\n",
      "        query (str, opcional): Consulta SQL SELECT. Por defecto es \"SELECT * FROM people\".\n",
      "\n",
      "    Retorna:\n",
      "        list: Lista de diccionarios con los resultados de la consulta.\n",
      "\n",
      "    Ejemplo de uso:\n",
      "        >>> get_people()\n",
      "        [{\"id\": 1, \"name\": \"John Doe\", \"age\": 30, \"profession\": \"Engineer\"}]\n",
      "\n",
      "        >>> get_people(\"SELECT name, profession FROM people WHERE age < 30\")\n",
      "        [{\"name\": \"Alice Smith\", \"profession\": \"Developer\"}]\n",
      "    \n",
      "get_all_people \n",
      "    Obtiene todas las personas de la base de datos.\n",
      "\n",
      "    Retorna:\n",
      "        list: Lista de todas las personas con sus detalles.\n",
      "    \n",
      "find_person_by_name \n",
      "    Busca personas por nombre en la base de datos.\n",
      "\n",
      "    Parámetros:\n",
      "        name (str): Nombre de la persona a buscar.\n",
      "\n",
      "    Retorna:\n",
      "        list: Lista de personas que coinciden con el nombre.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "tools = await mcp_tools.to_tool_list_async()\n",
    "for tool in tools:\n",
    "    print(tool.metadata.name, tool.metadata.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c97a15",
   "metadata": {},
   "source": [
    "### Definir el indicador del sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4f664873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "async def get_agent_ollama(tools: McpToolSpec):\n",
    "    tools = await tools.to_tool_list_async()\n",
    "    agent = FunctionAgent(\n",
    "        name=\"Agent\",\n",
    "        description=\"An agent that can work with Our Database software.\",\n",
    "        tools=tools,\n",
    "        llm=Ollama(model=\"llama3.1\", base_url=\"http://localhost:11434\"),\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "    )\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94754397",
   "metadata": {},
   "source": [
    "Este mensaje guía al LLM cuando necesita decidir cómo y cuándo llamar a las herramientas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f467bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\\\n",
    "Eres un asistente de IA para la llamada a herramientas.\n",
    "\n",
    "Antes de ayudar a un usuario, necesitas trabajar con herramientas para interactuar con nuestra base de datos.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad80d0",
   "metadata": {},
   "source": [
    "#### Helper function: `get_agent()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebbc54b",
   "metadata": {},
   "source": [
    "Crea un `FunctionAgent` conectado con la lista de herramientas MCP y el LLM elegido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d2a477aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.mcp import McpToolSpec\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "async def get_agent_openrouter(tools: McpToolSpec):\n",
    "    tools = await tools.to_tool_list_async()\n",
    "    agent = FunctionAgent(\n",
    "        name=\"Agent\",\n",
    "        description=\"An agent that can work with Our Database software.\",\n",
    "        tools=tools,\n",
    "        llm=OpenAI(\n",
    "            model=\"openai/gpt-4o-mini\",\n",
    "            api_key=\"API_KEY_HERE\",\n",
    "            api_base=\"https://openrouter.ai/api/v1\"\n",
    "        ),        \n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "    )\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "116f7de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "async def get_agent_ollama(tools: McpToolSpec):\n",
    "    tools = await tools.to_tool_list_async()\n",
    "    \n",
    "    agent = FunctionAgent(\n",
    "        name=\"Agent\",\n",
    "        description=\"An agent that can work with Our Database software.\",\n",
    "        tools=tools,\n",
    "        llm=Ollama(\n",
    "            model=\"llama3.1:latest\",  # o cualquier modelo que tengas en Ollama\n",
    "            request_timeout=360.0,\n",
    "        ),        \n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "    )\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b4baa",
   "metadata": {},
   "source": [
    "#### Helper function: `handle_user_message()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4406c5",
   "metadata": {},
   "source": [
    "Transmite llamadas a herramientas intermedias (para transparencia) y devuelve la respuesta final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "37960dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import (\n",
    "    FunctionAgent, \n",
    "    ToolCallResult, \n",
    "    ToolCall)\n",
    "\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "async def handle_user_message(\n",
    "    message_content: str,\n",
    "    agent: FunctionAgent,\n",
    "    agent_context: Context,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    handler = agent.run(message_content, ctx=agent_context)\n",
    "    async for event in handler.stream_events():\n",
    "        if verbose and type(event) == ToolCall:\n",
    "            print(f\"Calling tool {event.tool_name} with kwargs {event.tool_kwargs}\")\n",
    "        elif verbose and type(event) == ToolCallResult:\n",
    "            print(f\"Tool {event.tool_name} returned {event.tool_output}\")\n",
    "\n",
    "    response = await handler\n",
    "    return str(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82fbce",
   "metadata": {},
   "source": [
    "#### Initialize the MCP client and build the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271a983",
   "metadata": {},
   "source": [
    "Apunte el cliente al punto final SSE de su servidor MCP local (el valor predeterminado se muestra a continuación), cree el agente y configure el contexto del agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b0d7a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from llama_index.tools.mcp import BasicMCPClient, McpToolSpec\n",
    "\n",
    "# Set environment variables for longer timeouts\n",
    "os.environ['OLLAMA_REQUEST_TIMEOUT'] = '360'\n",
    "os.environ['OLLAMA_READ_TIMEOUT'] = '640'\n",
    "\n",
    "# Create MCP client\n",
    "mcp_client = BasicMCPClient(\"http://127.0.0.1:8000/sse\")\n",
    "mcp_tool = McpToolSpec(client=mcp_client)\n",
    "\n",
    "# Your existing code continues...\n",
    "agent = await get_agent_ollama(mcp_tool)\n",
    "agent_context = Context(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "df5cbe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  Add John Smith who is 30 years old and works as a Software Developer\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "model requires more system memory (29.7 GiB) than is available (28.3 GiB) (status code: 500)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[117]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUser: \u001b[39m\u001b[33m\"\u001b[39m, user_input)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m handle_user_message(user_input, agent, agent_context, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAgent: \u001b[39m\u001b[33m\"\u001b[39m, response)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[115]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mhandle_user_message\u001b[39m\u001b[34m(message_content, agent, agent_context, verbose)\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m verbose \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(event) == ToolCallResult:\n\u001b[32m     19\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTool \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent.tool_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent.tool_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m handler\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/anaconda/anaconda3/lib/python3.12/asyncio/futures.py:290\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/anaconda/anaconda3/lib/python3.12/asyncio/futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/llamaindex-mcp/.venv/lib/python3.12/site-packages/workflows/workflow.py:439\u001b[39m, in \u001b[36mWorkflow.run.<locals>._run_workflow\u001b[39m\u001b[34m(ctx)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_raised:\n\u001b[32m    436\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    437\u001b[39m     ctx.write_event_to_stream(StopEvent())\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_raised\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m we_done:\n\u001b[32m    442\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    443\u001b[39m     ctx.write_event_to_stream(StopEvent())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/anaconda/anaconda3/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/llamaindex-mcp/.venv/lib/python3.12/site-packages/workflows/context/context.py:822\u001b[39m, in \u001b[36mContext._step_worker\u001b[39m\u001b[34m(self, name, step, config, verbose, run_id, worker_id, resource_manager)\u001b[39m\n\u001b[32m    813\u001b[39m \u001b[38;5;28mself\u001b[39m.write_event_to_stream(\n\u001b[32m    814\u001b[39m     StepStateChanged(\n\u001b[32m    815\u001b[39m         name=name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    819\u001b[39m     )\n\u001b[32m    820\u001b[39m )\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     new_ev = \u001b[38;5;28;01mawait\u001b[39;00m instrumented_step(**kwargs)\n\u001b[32m    823\u001b[39m     kwargs.clear()\n\u001b[32m    824\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# exit the retrying loop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/llamaindex-mcp/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py:386\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28mself\u001b[39m.span_enter(\n\u001b[32m    379\u001b[39m     id_=id_,\n\u001b[32m    380\u001b[39m     bound_args=bound_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     tags=tags,\n\u001b[32m    384\u001b[39m )\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m.event(SpanDropEvent(span_id=id_, err_str=\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/llamaindex-mcp/.venv/lib/python3.12/site-packages/llama_index/core/agent/workflow/base_agent.py:390\u001b[39m, in \u001b[36mBaseWorkflowAgent.run_agent_step\u001b[39m\u001b[34m(self, ctx, ev)\u001b[39m\n\u001b[32m    387\u001b[39m user_msg_str = \u001b[38;5;28;01mawait\u001b[39;00m ctx.store.get(\u001b[33m\"\u001b[39m\u001b[33muser_msg_str\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    388\u001b[39m tools = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_tools(user_msg_str \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m agent_output = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.take_step(\n\u001b[32m    391\u001b[39m     ctx,\n\u001b[32m    392\u001b[39m     ev.input,\n\u001b[32m    393\u001b[39m     tools,\n\u001b[32m    394\u001b[39m     memory,\n\u001b[32m    395\u001b[39m )\n\u001b[32m    397\u001b[39m ctx.write_event_to_stream(agent_output)\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m agent_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/llamaindex-mcp/.venv/lib/python3.12/site-packages/llama_index/core/agent/workflow/function_agent.py:120\u001b[39m, in \u001b[36mFunctionAgent.take_step\u001b[39m\u001b[34m(self, ctx, llm_input, tools, memory)\u001b[39m\n\u001b[32m    115\u001b[39m ctx.write_event_to_stream(\n\u001b[32m    116\u001b[39m     AgentInput(\u001b[38;5;28minput\u001b[39m=current_llm_input, current_agent_name=\u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m    117\u001b[39m )\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.streaming:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     last_chat_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_streaming_response(\n\u001b[32m    121\u001b[39m         ctx, current_llm_input, tools\n\u001b[32m    122\u001b[39m     )\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    124\u001b[39m     last_chat_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_response(current_llm_input, tools)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/llamaindex-mcp/.venv/lib/python3.12/site-packages/llama_index/core/agent/workflow/function_agent.py:75\u001b[39m, in \u001b[36mFunctionAgent._get_streaming_response\u001b[39m\u001b[34m(self, ctx, current_llm_input, tools)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# last_chat_response will be used later, after the loop.\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# We initialize it so it's valid even when 'response' is empty\u001b[39;00m\n\u001b[32m     74\u001b[39m last_chat_response = ChatResponse(message=ChatMessage())\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m last_chat_response \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[32m     76\u001b[39m     tool_calls = \u001b[38;5;28mself\u001b[39m.llm.get_tool_calls_from_response(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     77\u001b[39m         last_chat_response, error_on_no_tool_call=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     78\u001b[39m     )\n\u001b[32m     79\u001b[39m     raw = (\n\u001b[32m     80\u001b[39m         last_chat_response.raw.model_dump()\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(last_chat_response.raw, BaseModel)\n\u001b[32m     82\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m last_chat_response.raw\n\u001b[32m     83\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/llamaindex-mcp/.venv/lib/python3.12/site-packages/llama_index/core/llms/callbacks.py:89\u001b[39m, in \u001b[36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_async_llm_chat.<locals>.wrapped_gen\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     87\u001b[39m last_response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m f_return_val:\n\u001b[32m     90\u001b[39m         dispatcher.event(\n\u001b[32m     91\u001b[39m             LLMChatInProgressEvent(\n\u001b[32m     92\u001b[39m                 messages=messages,\n\u001b[32m   (...)\u001b[39m\u001b[32m     95\u001b[39m             )\n\u001b[32m     96\u001b[39m         )\n\u001b[32m     97\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m cast(ChatResponse, x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/llamaindex-mcp/.venv/lib/python3.12/site-packages/llama_index/llms/ollama/base.py:483\u001b[39m, in \u001b[36mOllama.astream_chat.<locals>.gen\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    480\u001b[39m seen_tool_calls = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    481\u001b[39m all_tool_calls = []\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[32m    484\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m r[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    485\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/llamaindex-mcp/.venv/lib/python3.12/site-packages/ollama/_client.py:741\u001b[39m, in \u001b[36mAsyncClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    740\u001b[39m   \u001b[38;5;28;01mawait\u001b[39;00m e.response.aread()\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r.aiter_lines():\n\u001b[32m    744\u001b[39m   part = json.loads(line)\n",
      "\u001b[31mResponseError\u001b[39m: model requires more system memory (29.7 GiB) than is available (28.3 GiB) (status code: 500)"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"Enter your message: \")\n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "    print(\"User: \", user_input)\n",
    "    response = await handle_user_message(user_input, agent, agent_context, verbose=True)\n",
    "    print(\"Agent: \", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex-mcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
